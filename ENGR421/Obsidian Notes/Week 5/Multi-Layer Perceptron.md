Lecture 10 or 12?

In the lecture notes: the first two pages show the difference between single layer perceptron and two layer perceptron. the Z layer in the second example helps us map the corner blue point to the other corner blue point and than it lets us draw a line on it, resulting in a linear system solution.

**Questions:** 
	I thought in each layer each node had its own bias, not a single bias for the whole layer.
	Isn't the domain of solutions so big?, then how do we find how many layers are needed, do we try as much as we can?
	All connected networks: are there also randomly connected or half connected networks?
	Yes: CNN and RNN

The hidden layer's function must be non-linear, why?
	Because if both layers are linear the output layer will also be linear, resulting in no benefit to useing the hidden layer.
	s1-> sigmoid, s2 -> softmax
	$z_h$                   $y_c$
	




